{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM-SNGAT Demonstration Notebook\n",
    "\n",
    "This notebook demonstrates the LLM-Simulated Nonequivalent Groups with Anchor Test (LLM-SNGAT) methodology for test equating.\n",
    "\n",
    "## Overview\n",
    "\n",
    "LLM-SNGAT addresses the challenge of test equating when traditional anchor items are not available by:\n",
    "1. Using LLMs to simulate authentic test-taker responses\n",
    "2. Transforming non-anchor equating problems into common-item nonequivalent groups designs\n",
    "3. Applying traditional psychometric equating methods (Tucker and Levine)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Import LLM-SNGAT modules\n",
    "from llm_sngat import (\n",
    "    AQuaDatasetLoader, StudentSimulator, LLMResponseSimulator,\n",
    "    LLMSNGATProcessor, ResultAnalyzer\n",
    ")\n",
    "from config import Config\n",
    "from utils import setup_logging, generate_sample_aqua_problems\n",
    "\n",
    "# Set up plotting style\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "sns.set_theme(style=\"ticks\", palette=\"pastel\")\n",
    "\n",
    "# Configure for notebook\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "print(\"LLM-SNGAT Demo - Environment Ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Initialize Components and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize configuration\n",
    "config = Config()\n",
    "config.create_directories()\n",
    "\n",
    "# Show configuration\n",
    "print(\"Simulation Configuration:\")\n",
    "print(f\"  Students per group: {config.SIMULATION.n_students}\")\n",
    "print(f\"  Test form size: {config.SIMULATION.form_size}\")\n",
    "print(f\"  Math ability: μ={config.SIMULATION.math_ability_mean}, σ={config.SIMULATION.math_ability_std}\")\n",
    "print(f\"  Common item sizes: {config.SIMULATION.common_item_sizes}\")\n",
    "print(f\"\\nAvailable models: {list(config.MODELS.keys())}\")\n",
    "\n",
    "# Check API availability\n",
    "api_status = config.validate_api_keys()\n",
    "print(\"\\nAPI Key Status:\")\n",
    "for model, available in api_status.items():\n",
    "    status = \"✓ Available\" if available else \"✗ Not available\"\n",
    "    print(f"  Q{i+1}: {response} ({'✓' if is_correct else '✗'}) [Correct: {problem.correct}]")
print(f"First 5 items: {correct_count}/5 correct")
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Initial Score Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create score distribution plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Score histograms\n",
    "ax1.hist(scores_x_initial, alpha=0.7, label='Form X', bins=20, color='#1f4e79')\n",
    "ax1.hist(scores_y_initial, alpha=0.7, label='Form Y', bins=20, color='#c5504b')\n",
    "ax1.set_xlabel('Test Score', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Initial Score Distributions', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Ability vs Score scatter\n",
    "ax2.scatter(abilities_x, scores_x_initial, alpha=0.6, label='Form X', s=50, color='#1f4e79')\n",
    "ax2.scatter(abilities_y, scores_y_initial, alpha=0.6, label='Form Y', s=50, color='#c5504b')\n",
    "ax2.set_xlabel('Math Ability', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Test Score', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Ability vs Score Relationship', fontsize=14, fontweight='bold')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Add correlation information\n",
    "corr_x = np.corrcoef(abilities_x, scores_x_initial)[0, 1]\n",
    "corr_y = np.corrcoef(abilities_y, scores_y_initial)[0, 1]\n",
    "ax2.text(0.05, 0.95, f'Form X correlation: {corr_x:.3f}\\nForm Y correlation: {corr_y:.3f}', \n",
    "         transform=ax2.transAxes, verticalalignment='top', \n",
    "         bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Ability-Score Correlations:\")\n",
    "print(f\"  Form X: r = {corr_x:.3f}\")\n",
    "print(f\"  Form Y: r = {corr_y:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stage 2: LLM-SNGAT Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different common item sizes\n",
    "equating_results_all = {}\n",
    "\n",
    "print(\"Running LLM-SNGAT for different common item sizes...\\n\")\n",
    "\n",
    "for n_common in config.SIMULATION.common_item_sizes:\n",
    "    print(f\"Processing with {n_common} common items ({(n_common*2)/config.SIMULATION.form_size*100:.0f}% anchor proportion)\")\n",
    "    \n",
    "    # Stage 2: Create common items and simulate responses\n",
    "    new_form_x, new_form_y, common_x, common_y = processor.stage_two_simulation(\n",
    "        form_x, form_y, students_x, students_y, n_common\n",
    "    )\n",
    "    \n",
    "    print(f\"  Created forms: X={len(new_form_x)} items, Y={len(new_form_y)} items\")\n",
    "    \n",
    "    # Calculate scores for new forms\n",
    "    scores_x = processor.calculate_scores(students_x, new_form_x, 'responses_x')\n",
    "    scores_y = processor.calculate_scores(students_y, new_form_y, 'responses_y')\n",
    "    \n",
    "    # Extract common item scores\n",
    "    common_scores_x = []\n",
    "    common_scores_y = []\n",
    "    \n",
    "    for student in students_x:\n",
    "        score = sum(1 for i in range(len(form_x), len(new_form_x)) \n",
    "                   if i in student.responses_x and \n",
    "                   student.responses_x[i] == new_form_x[i].correct)\n",
    "        common_scores_x.append(score)\n",
    "    \n",
    "    for student in students_y:\n",
    "        score = sum(1 for i in range(len(form_y), len(new_form_y)) \n",
    "                   if i in student.responses_y and \n",
    "                   student.responses_y[i] == new_form_y[i].correct)\n",
    "        common_scores_y.append(score)\n",
    "    \n",
    "    # Perform equating\n",
    "    equating_results = processor.tucker_levine_equating(\n",
    "        scores_x, scores_y, common_scores_x, common_scores_y\n",
    "    )\n",
    "    \n",
    "    # Calculate standard errors\n",
    "    analyzer = ResultAnalyzer()\n",
    "    standard_errors = analyzer.calculate_standard_errors(\n",
    "        scores_x, scores_y, equating_results\n",
    "    )\n",
    "    \n",
    "    # Store results\n",
    "    equating_results_all[n_common] = {\n",
    "        'tucker_se': standard_errors['tucker_se'],\n",
    "        'levine_se': standard_errors['levine_se'],\n",
    "        'scores_x': scores_x,\n",
    "        'scores_y': scores_y,\n",
    "        'common_scores_x': common_scores_x,\n",
    "        'common_scores_y': common_scores_y,\n",
    "        'equating_results': equating_results\n",
    "    }\n",
    "    \n",
    "    print(f\"  Tucker SE: {standard_errors['tucker_se']:.4f}\")\n",
    "    print(f\"  Levine SE: {standard_errors['levine_se']:.4f}\")\n",
    "    print(f\"  Common item correlation: {np.corrcoef(common_scores_x, common_scores_y)[0,1]:.3f}\")\n",
    "    print()\n",
    "\n",
    "print(\"LLM-SNGAT processing complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create Results Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for n_common, results in equating_results_all.items():\n",
    "    anchor_prop = f\"{(n_common*2)/config.SIMULATION.form_size*100:.0f}%\"\n",
    "    summary_data.append({\n",
    "        'Common Items': n_common,\n",
    "        'Anchor Proportion': anchor_prop,\n",
    "        'Tucker SE': results['tucker_se'],\n",
    "        'Levine SE': results['levine_se'],\n",
    "        'Common Correlation': np.corrcoef(results['common_scores_x'], results['common_scores_y'])[0,1],\n",
    "        'Score Diff (X-Y)': np.mean(results['scores_x']) - np.mean(results['scores_y'])\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "summary_df = summary_df.round(4)\n",
    "\n",
    "print(\"LLM-SNGAT Results Summary:\")\n",
    "display(summary_df)\n",
    "\n",
    "# Find optimal configurations\n",
    "best_tucker = summary_df.loc[summary_df['Tucker SE'].idxmin()]\n",
    "best_levine = summary_df.loc[summary_df['Levine SE'].idxmin()]\n",
    "\n",
    "print(f\"\\nOptimal Configurations:\")\n",
    "print(f\"Tucker Equating: {best_tucker['Common Items']} common items (SE = {best_tucker['Tucker SE']:.4f})\")\n",
    "print(f\"Levine Equating: {best_levine['Common Items']} common items (SE = {best_levine['Levine SE']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Standard Error Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create standard error comparison plot\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "common_items = summary_df['Common Items'].values\n",
    "tucker_ses = summary_df['Tucker SE'].values\n",
    "levine_ses = summary_df['Levine SE'].values\n",
    "\n",
    "# Plot lines with markers\n",
    "plt.plot(common_items, tucker_ses, 'o-', label='Tucker Equating', \n",
    "         linewidth=2.5, markersize=8, color='#1f4e79')\n",
    "plt.plot(common_items, levine_ses, 's--', label='Levine Equating', \n",
    "         linewidth=2.5, markersize=7, color='#c5504b')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel('Common Items Size', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Standard Error', fontsize=16, fontweight='bold')\n",
    "plt.title('Standard Error vs Common Item Size (LLM-SNGAT)', \n",
    "          fontsize=18, fontweight='bold', pad=20)\n",
    "plt.legend(fontsize=14, frameon=True, fancybox=True, shadow=True)\n",
    "plt.grid(True, linestyle='--', alpha=0.3, linewidth=0.8)\n",
    "\n",
    "# Add value annotations\n",
    "for i, (x, y1, y2) in enumerate(zip(common_items, tucker_ses, levine_ses)):\n",
    "    plt.annotate(f'{y1:.3f}', (x, y1), textcoords=\"offset points\", \n",
    "                xytext=(0,10), ha='center', fontsize=10)\n",
    "    plt.annotate(f'{y2:.3f}', (x, y2), textcoords=\"offset points\", \n",
    "                xytext=(0,-15), ha='center', fontsize=10)\n",
    "\n",
    "plt.xticks(fontsize=14)\n",
    "plt.yticks(fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlation with common item size\n",
    "tucker_corr = np.corrcoef(common_items, tucker_ses)[0, 1]\n",
    "levine_corr = np.corrcoef(common_items, levine_ses)[0, 1]\n",
    "\n",
    "print(f\"\\nCorrelation between common item size and standard error:\")\n",
    "print(f\"Tucker: r = {tucker_corr:.3f}\")\n",
    "print(f\"Levine: r = {levine_corr:.3f}\")\n",
    "\n",
    "if tucker_corr < 0:\n",
    "    print(\"→ Tucker equating benefits from more common items\")\n",
    "if levine_corr < 0:\n",
    "    print(\"→ Levine equating benefits from more common items\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Detailed Analysis for Optimal Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the configuration with lowest average standard error\n",
    "avg_ses = [(tucker + levine)/2 for tucker, levine in zip(tucker_ses, levine_ses)]\n",
    "optimal_idx = np.argmin(avg_ses)\n",
    "optimal_n_common = common_items[optimal_idx]\n",
    "\n",
    "print(f\"Detailed analysis for optimal configuration: {optimal_n_common} common items\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get results for optimal configuration\n",
    "optimal_results = equating_results_all[optimal_n_common]\n",
    "scores_x = optimal_results['scores_x']\n",
    "scores_y = optimal_results['scores_y']\n",
    "common_scores_x = optimal_results['common_scores_x']\n",
    "common_scores_y = optimal_results['common_scores_y']\n",
    "equating_results = optimal_results['equating_results']\n",
    "\n",
    "# Detailed statistics\n",
    "print(f\"\\nScore Statistics:\")\n",
    "print(f\"Form X: μ={np.mean(scores_x):.2f}, σ={np.std(scores_x):.2f}\")\n",
    "print(f\"Form Y: μ={np.mean(scores_y):.2f}, σ={np.std(scores_y):.2f}\")\n",
    "print(f\"Difference: {np.mean(scores_x) - np.mean(scores_y):.2f}\")\n",
    "\n",
    "print(f\"\\nCommon Item Statistics:\")\n",
    "print(f\"Common X: μ={np.mean(common_scores_x):.2f}, σ={np.std(common_scores_x):.2f}\")\n",
    "print(f\"Common Y: μ={np.mean(common_scores_y):.2f}, σ={np.std(common_scores_y):.2f}\")\n",
    "print(f\"Correlation: {np.corrcoef(common_scores_x, common_scores_y)[0,1]:.3f}\")\n",
    "\n",
    "print(f\"\\nEquating Parameters:\")\n",
    "tucker_params = equating_results['tucker']\n",
    "levine_params = equating_results['levine']\n",
    "print(f\"Tucker - Slope: {tucker_params['slope']:.4f}, Intercept: {tucker_params['intercept']:.4f}\")\n",
    "print(f\"Levine - Slope: {levine_params['slope']:.4f}, Intercept: {levine_params['intercept']:.4f}\")\n",
    "\n",
    "# Create detailed visualization\n",
    "analyzer.plot_results(students_x, students_y, scores_x, scores_y, equating_results)\n",
    "\n",
    "print(f\"\\nStandard Errors:\")\n",
    "print(f\"Tucker: {optimal_results['tucker_se']:.4f}\")\n",
    "print(f\"Levine: {optimal_results['levine_se']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Compare with Traditional Methods (Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate traditional equating with real anchor items for comparison\n",
    "print(\"Comparison with Traditional Anchor-Based Equating\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Simulate traditional method by using the same items as anchors in both forms\n",
    "traditional_results = {}\n",
    "\n",
    "for n_anchor in config.SIMULATION.common_item_sizes:\n",
    "    # In traditional method, anchor items are the same in both forms\n",
    "    # We simulate this by creating shared anchor items\n",
    "    \n",
    "    # Select anchor items from Form X\n",
    "    anchor_indices = np.random.choice(len(form_x), n_anchor, replace=False)\n",
    "    anchor_items = [form_x[i] for i in anchor_indices]\n",
    "    \n",
    "    # Calculate anchor scores for both groups on the same items\n",
    "    anchor_scores_x = []\n",
    "    anchor_scores_y = []\n",
    "    \n",
    "    for student in students_x:\n",
    "        score = sum(1 for i, item in enumerate(anchor_items)\n",
    "                   if student.responses_x.get(anchor_indices[i], '') == item.correct)\n",
    "        anchor_scores_x.append(score)\n",
    "    \n",
    "    # Simulate Group Y responses to anchor items\n",
    "    for student in students_y:\n",
    "        score = 0\n",
    "        for item in anchor_items:\n",
    "            # Simulate response based on ability\n",
    "            if np.random.random() < student.math_ability / 100.0:\n",
    "                score += 1\n",
    "        anchor_scores_y.append(score)\n",
    "    \n",
    "    # Use original total scores and anchor scores for traditional equating\n",
    "    traditional_equating = processor.tucker_levine_equating(\n",
    "        scores_x_initial, scores_y_initial, anchor_scores_x, anchor_scores_y\n",
    "    )\n",
    "    \n",
    "    traditional_se = analyzer.calculate_standard_errors(\n",
    "        scores_x_initial, scores_y_initial, traditional_equating\n",
    "    )\n",
    "    \n",
    "    traditional_results[n_anchor] = {\n",
    "        'tucker_se': traditional_se['tucker_se'],\n",
    "        'levine_se': traditional_se['levine_se']\n",
    "    }\n",
    "\n",
    "# Create comparison table\n",
    "comparison_data = []\n",
    "for n_items in config.SIMULATION.common_item_sizes:\n",
    "    llm_sngat = equating_results_all[n_items]\n",
    "    traditional = traditional_results[n_items]\n",
    "    \n",
    "    comparison_data.append({\n",
    "        'Common Items': n_items,\n",
    "        'LLM-SNGAT Tucker': llm_sngat['tucker_se'],\n",
    "        'Traditional Tucker': traditional['tucker_se'],\n",
    "        'Tucker Improvement': traditional['tucker_se'] - llm_sngat['tucker_se'],\n",
    "        'LLM-SNGAT Levine': llm_sngat['levine_se'],\n",
    "        'Traditional Levine': traditional['levine_se'],\n",
    "        'Levine Improvement': traditional['levine_se'] - llm_sngat['levine_se']\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(4)\n",
    "\n",
    "print(\"\\nMethod Comparison (Standard Errors):\")\n",
    "display(comparison_df[['Common Items', 'LLM-SNGAT Tucker', 'Traditional Tucker', \n",
    "                     'LLM-SNGAT Levine', 'Traditional Levine']])\n",
    "\n",
    "# Calculate average improvements\n",
    "avg_tucker_improvement = comparison_df['Tucker Improvement'].mean()\n",
    "avg_levine_improvement = comparison_df['Levine Improvement'].mean()\n",
    "\n",
    "print(f\"\\nAverage Standard Error Differences (Traditional - LLM-SNGAT):\")\n",
    "print(f\"Tucker: {avg_tucker_improvement:+.4f} {'(LLM-SNGAT better)' if avg_tucker_improvement > 0 else '(Traditional better)'}\")\n",
    "print(f\"Levine: {avg_levine_improvement:+.4f} {'(LLM-SNGAT better)' if avg_levine_improvement > 0 else '(Traditional better)'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Summary and Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LLM-SNGAT DEMONSTRATION SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n📊 Experiment Configuration:\")\n",
    "print(f\"   • Students per group: {config.SIMULATION.n_students}\")\n",
    "print(f\"   • Test form size: {config.SIMULATION.form_size} items\")\n",
    "print(f\"   • Common item sizes tested: {config.SIMULATION.common_item_sizes}\")\n",
    "print(f\"   • Anchor proportions: {[f'{(n*2)/config.SIMULATION.form_size*100:.0f}%' for n in config.SIMULATION.common_item_sizes]}\")\n",
    "\n",
    "print(f\"\\n🎯 Key Findings:\")\n",
    "\n",
    "# Best configurations\n",
    "best_tucker_n = summary_df.loc[summary_df['Tucker SE'].idxmin(), 'Common Items']\n",
    "best_levine_n = summary_df.loc[summary_df['Levine SE'].idxmin(), 'Common Items']\n",
    "best_tucker_se = summary_df['Tucker SE'].min()\n",
    "best_levine_se = summary_df['Levine SE'].min()\n",
    "\n",
    "print(f\"   • Best Tucker configuration: {best_tucker_n} common items (SE = {best_tucker_se:.4f})\")\n",
    "print(f\"   • Best Levine configuration: {best_levine_n} common items (SE = {best_levine_se:.4f})\")\n",
    "\n",
    "# Trends\n",
    "if tucker_corr < -0.5:\n",
    "    print(f\"   • Tucker equating shows strong improvement with more common items (r = {tucker_corr:.3f})\")\n",
    "elif tucker_corr < 0:\n",
    "    print(f\"   • Tucker equating shows moderate improvement with more common items (r = {tucker_corr:.3f})\")\n",
    "\n",
    "if levine_corr < -0.5:\n",
    "    print(f\"   • Levine equating shows strong improvement with more common items (r = {levine_corr:.3f})\")\n",
    "elif levine_corr < 0:\n",
    "    print(f\"   • Levine equating shows moderate improvement with more common items (r = {levine_corr:.3f})\")\n",
    "\n",
    "# Method comparison\n",
    "if avg_tucker_improvement > 0.001:\n",
    "    print(f\"   • LLM-SNGAT outperforms traditional Tucker equating (avg. improvement: {avg_tucker_improvement:.4f})\")\n",
    "elif avg_tucker_improvement < -0.001:\n",
    "    print(f\"   • Traditional Tucker equating outperforms LLM-SNGAT (avg. difference: {abs(avg_tucker_improvement):.4f})\")\n",
    "else:\n",
    "    print(f\"   • LLM-SNGAT and traditional Tucker equating perform similarly\")\n",
    "\n",
    "if avg_levine_improvement > 0.001:\n",
    "    print(f\"   • LLM-SNGAT outperforms traditional Levine equating (avg. improvement: {avg_levine_improvement:.4f})\")\n",
    "elif avg_levine_improvement < -0.001:\n",
    "    print(f\"   • Traditional Levine equating outperforms LLM-SNGAT (avg. difference: {abs(avg_levine_improvement):.4f})\")\n",
    "else:\n",
    "    print(f\"   • LLM-SNGAT and traditional Levine equating perform similarly\")\n",
    "\n",
    "print(f\"\\n💡 Practical Implications:\")\n",
    "print(f\"   • LLM-SNGAT enables test equating without pre-existing anchor items\")\n",
    "print(f\"   • Method is particularly valuable for new test development\")\n",
    "print(f\"   • Optimal anchor proportion appears to be around {(optimal_n_common*2)/config.SIMULATION.form_size*100:.0f}%\")\n",
    "print(f\"   • Both Tucker and Levine methods can be effectively implemented\")\n",
    "\n",
    "print(f\"\\n⚠️  Limitations of this demonstration:\")\n",
    "print(f\"   • Uses simulated rather than real LLM responses\")\n",
    "print(f\"   • Limited to one replication per condition\")\n",
    "print(f\"   • Simplified problem generation\")\n",
    "print(f\"   • Standard errors calculated using simplified approximations\")\n",
    "\n",
    "print(f\"\\n🔬 Next steps for real implementation:\")\n",
    "print(f\"   • Integrate actual LLM APIs (GPT-4, Claude, etc.)\")\n",
    "print(f\"   • Use real AQua dataset or domain-specific problems\")\n",
    "print(f\"   • Implement full delta method for standard error calculation\")\n",
    "print(f\"   • Conduct multiple replications for robust results\")\n",
    "print(f\"   • Validate against known equating benchmarks\")\n",
    "\n",
    "print(\"\\n✅ Demonstration completed successfully!\")\n",
    "print(\"\\n📁 All results and visualizations are available in the notebook output.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}{model}: {status}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load or Generate Test Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset loader with sample problems\n",
    "dataset_loader = AQuaDatasetLoader()\n",
    "\n",
    "print(f\"Total problems available: {len(dataset_loader.problems)}\")\n",
    "\n",
    "# Show example problems\n",
    "print(\"\\nExample problems:\")\n",
    "for i, problem in enumerate(dataset_loader.problems[:3]):\n",
    "    print(f\"\\nProblem {i+1}:\")\n",
    "    print(f\"Question: {problem.question}\")\n",
    "    print(f\"Options: {problem.options}\")\n",
    "    print(f\"Correct: {problem.correct}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create Test Forms and Simulate Students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test forms\n",
    "form_x, form_y = dataset_loader.create_test_forms(config.SIMULATION.form_size)\n",
    "\n",
    "print(f\"Form X: {len(form_x)} items\")\n",
    "print(f\"Form Y: {len(form_y)} items\")\n",
    "\n",
    "# Generate simulated students\n",
    "student_simulator = StudentSimulator()\n",
    "students_x = student_simulator.generate_students(config.SIMULATION.n_students)\n",
    "students_y = student_simulator.generate_students(config.SIMULATION.n_students)\n",
    "\n",
    "print(f\"\\nGenerated {len(students_x)} students for Group X\")\n",
    "print(f\"Generated {len(students_y)} students for Group Y\")\n",
    "\n",
    "# Show student characteristics\n",
    "abilities_x = [s.math_ability for s in students_x]\n",
    "abilities_y = [s.math_ability for s in students_y]\n",
    "\n",
    "print(f\"\\nGroup X ability: μ={np.mean(abilities_x):.2f}, σ={np.std(abilities_x):.2f}\")\n",
    "print(f\"Group Y ability: μ={np.mean(abilities_y):.2f}, σ={np.std(abilities_y):.2f}\")\n",
    "\n",
    "# Show example students\n",
    "print(\"\\nExample students from Group X:\")\n",
    "for i, student in enumerate(students_x[:3]):\n",
    "    print(f\"{i+1}. {student.name} ({student.gender}) from {student.city}, {student.province}\")\n",
    "    print(f\"   Math ability: {student.math_ability:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualize Student Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization of student characteristics\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Ability distributions\n",
    "ax1.hist(abilities_x, alpha=0.7, label='Group X', bins=20, color='#1f4e79')\n",
    "ax1.hist(abilities_y, alpha=0.7, label='Group Y', bins=20, color='#c5504b')\n",
    "ax1.set_xlabel('Math Ability', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Frequency', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Distribution of Math Abilities', fontsize=14, fontweight='bold')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Gender distribution\n",
    "gender_x = [s.gender for s in students_x]\n",
    "gender_y = [s.gender for s in students_y]\n",
    "gender_counts_x = pd.Series(gender_x).value_counts()\n",
    "gender_counts_y = pd.Series(gender_y).value_counts()\n",
    "\n",
    "x_pos = np.arange(len(gender_counts_x))\n",
    "width = 0.35\n",
    "\n",
    "ax2.bar(x_pos - width/2, gender_counts_x.values, width, label='Group X', color='#1f4e79', alpha=0.7)\n",
    "ax2.bar(x_pos + width/2, gender_counts_y.values, width, label='Group Y', color='#c5504b', alpha=0.7)\n",
    "ax2.set_xlabel('Gender', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('Count', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Gender Distribution', fontsize=14, fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(gender_counts_x.index)\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot of abilities\n",
    "ax3.boxplot([abilities_x, abilities_y], labels=['Group X', 'Group Y'], \n",
    "           patch_artist=True, boxprops=dict(facecolor='lightblue', alpha=0.7))\n",
    "ax3.set_ylabel('Math Ability', fontsize=12, fontweight='bold')\n",
    "ax3.set_title('Ability Distribution Comparison', fontsize=14, fontweight='bold')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "# Geographic distribution (top provinces)\n",
    "provinces_x = [s.province for s in students_x]\n",
    "province_counts = pd.Series(provinces_x).value_counts().head(8)\n",
    "ax4.barh(range(len(province_counts)), province_counts.values, color='#70ad47', alpha=0.7)\n",
    "ax4.set_yticks(range(len(province_counts)))\n",
    "ax4.set_yticklabels(province_counts.index)\n",
    "ax4.set_xlabel('Count', fontsize=12, fontweight='bold')\n",
    "ax4.set_title('Top Provinces (Group X)', fontsize=14, fontweight='bold')\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Stage 1: Initial Response Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize LLM simulator (using simulation mode for demo)\n",
    "simulator = LLMResponseSimulator()\n",
    "\n",
    "# Simulate responses to original forms\n",
    "print(\"Simulating responses to Form X...\")\n",
    "for student in students_x:\n",
    "    student.responses_x = simulator.simulate_responses(student, form_x)\n",
    "\n",
    "print(\"Simulating responses to Form Y...\")\n",
    "for student in students_y:\n",
    "    student.responses_y = simulator.simulate_responses(student, form_y)\n",
    "\n",
    "print(\"Response simulation complete!\")\n",
    "\n",
    "# Calculate initial scores\n",
    "processor = LLMSNGATProcessor(dataset_loader, simulator)\n",
    "scores_x_initial = processor.calculate_scores(students_x, form_x, 'responses_x')\n",
    "scores_y_initial = processor.calculate_scores(students_y, form_y, 'responses_y')\n",
    "\n",
    "print(f\"\\nInitial Score Statistics:\")\n",
    "print(f\"Form X: μ={np.mean(scores_x_initial):.2f}, σ={np.std(scores_x_initial):.2f}, range=[{min(scores_x_initial)}, {max(scores_x_initial)}]\")\n",
    "print(f\"Form Y: μ={np.mean(scores_y_initial):.2f}, σ={np.std(scores_y_initial):.2f}, range=[{min(scores_y_initial)}, {max(scores_y_initial)}]\")\n",
    "\n",
    "# Show some example responses\n",
    "print(\"\\nExample responses from first student in Group X:\")\n",
    "example_student = students_x[0]\n",
    "print(f\"Student: {example_student.name} (ability: {example_student.math_ability:.1f})\")\n",
    "correct_count = 0\n",
    "for i, (response, problem) in enumerate(zip(\n",
    "    [example_student.responses_x.get(j, 'No response') for j in range(5)], \n",
    "    form_x[:5]\n",
    ")):\n",
    "    is_correct = response == problem.correct\n",
    "    if is_correct:\n",
    "        correct_count += 1\n",
    "    print(f\"  