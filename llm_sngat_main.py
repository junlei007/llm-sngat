"""
LLM-Simulated Nonequivalent Groups with Anchor Test (LLM-SNGAT) Implementation

This implementation reproduces the methodology described in the research paper for 
test equating using LLM-simulated response patterns.
"""

import json
import random
import numpy as np
from typing import List, Dict, Any, Tuple
from faker import Faker
import openai
from dataclasses import dataclass
import pandas as pd
from scipy import stats
import matplotlib.pyplot as plt
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“å’Œç»˜å›¾é£Žæ ¼
plt.rcParams['font.sans-serif'] = ['SimHei']  # æŒ‡å®šé»˜è®¤å­—ä½“
plt.rcParams['axes.unicode_minus'] = False   # è§£å†³ä¿å­˜å›¾åƒæ˜¯è´Ÿå·'-'æ˜¾ç¤ºä¸ºæ–¹å—çš„é—®é¢˜
sns.set_theme(style="ticks", palette="pastel")

# å°è¯•ä½¿ç”¨ç§‘å­¦ç»˜å›¾é£Žæ ¼ï¼ˆå¦‚æžœå®‰è£…äº†scienceplotsï¼‰
try:
    plt.style.use('science')
except:
    print("Warning: 'science' style not available. Using default style.")

# å…¨å±€è®¾ç½® - å¢žå¤§å­—ä½“å¤§å°
plt.rcParams['font.family'] = 'Helvetica'  # æˆ– 'Arial' Helvetica
plt.rcParams['font.size'] = 14  # å¢žå¤§åŸºç¡€å­—å·
plt.rcParams['axes.linewidth'] = 1.2  # å¢žç²—åæ ‡è½´çº¿æ¡
plt.rcParams['xtick.major.size'] = 6  # å¢žå¤§åˆ»åº¦çº¿
plt.rcParams['ytick.major.size'] = 6
plt.rcParams['xtick.minor.size'] = 3
plt.rcParams['ytick.minor.size'] = 3

@dataclass
class MathProblem:
    """Represents a mathematical problem from AQua dataset"""
    question: str
    options: List[str]
    correct: str
    
@dataclass
class SimulatedStudent:
    """Represents a simulated student with demographic and cognitive characteristics"""
    name: str
    gender: str
    province: str
    city: str
    math_ability: float
    responses_x: Dict[int, str] = None
    responses_y: Dict[int, str] = None
    
class AQuaDatasetLoader:
    """Loads and manages AQua dataset problems"""
    
    def __init__(self, dataset_path: str = None):
        self.problems = []
        self.dataset_info = {}
        
        if dataset_path:
            self.load_dataset(dataset_path)
        else:
            # Generate sample problems for demonstration
            print("No dataset path provided. Using generated sample problems.")
            print("To use real AQuA data, download from: https://github.com/google-deepmind/AQuA")
            self._generate_sample_problems()
    
    def load_dataset(self, path: str):
        """Load AQuA dataset from various formats"""
        import os
        from pathlib import Path
        
        path = Path(path)
        
        if path.is_file() and path.suffix == '.json':
            # Load from JSON file (original AQuA format)
            self._load_from_json(path)
        elif path.is_dir():
            # Load from Hugging Face datasets directory
            self._load_from_huggingface_dir(path)
        else:
            # Try to load as Hugging Face dataset name
            self._load_from_huggingface_name(str(path))
    
    def _load_from_json(self, filepath: Path):
        """Load from original AQuA JSON format"""
        print(f"Loading AQuA dataset from: {filepath}")
        
        try:
            with open(filepath, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"Loaded {len(data)} problems from JSON file")
            
            for i, item in enumerate(data):
                try:
                    # Validate required fields
                    required_fields = ['question', 'options', 'correct']
                    if not all(field in item for field in required_fields):
                        print(f"Warning: Item {i} missing required fields. Skipping.")
                        continue
                    
                    # Clean and validate options
                    options = item['options']
                    if not isinstance(options, list) or len(options) != 5:
                        print(f"Warning: Item {i} has invalid options format. Skipping.")
                        continue
                    
                    # Ensure options are properly formatted
                    formatted_options = []
                    for j, option in enumerate(options):
                        option_letter = chr(65 + j)  # A, B, C, D, E
                        if not option.startswith(f"{option_letter})"):
                            # Add option letter if missing
                            formatted_options.append(f"{option_letter}){option}")
                        else:
                            formatted_options.append(option)
                    
                    problem = MathProblem(
                        question=item['question'].strip(),
                        options=formatted_options,
                        correct=item['correct'].strip().upper()
                    )
                    
                    # Validate correct answer
                    if problem.correct not in ['A', 'B', 'C', 'D', 'E']:
                        print(f"Warning: Item {i} has invalid correct answer '{problem.correct}'. Skipping.")
                        continue
                    
                    self.problems.append(problem)
                    
                except Exception as e:
                    print(f"Warning: Error processing item {i}: {e}")
                    continue
            
            self.dataset_info = {
                'source': 'AQuA-RAT Original',
                'path': str(filepath),
                'total_problems': len(self.problems),
                'format': 'JSON'
            }
            
            print(f"Successfully loaded {len(self.problems)} valid problems")
            
        except Exception as e:
            print(f"Error loading dataset from {filepath}: {e}")
            print("Falling back to generated sample problems...")
            self._generate_sample_problems()
    
    def _load_from_huggingface_dir(self, dirpath: Path):
        """Load from Hugging Face datasets directory structure"""
        try:
            from datasets import load_from_disk
            
            print(f"Loading AQuA dataset from Hugging Face directory: {dirpath}")
            dataset = load_from_disk(str(dirpath))
            
            # Use train split by default
            if 'train' in dataset:
                data = dataset['train']
            else:
                data = dataset
            
            print(f"Loaded {len(data)} problems from Hugging Face dataset")
            
            for i, item in enumerate(data):
                try:
                    problem = MathProblem(
                        question=item['question'].strip(),
                        options=item['options'],
                        correct=item['correct'].strip().upper()
                    )
                    self.problems.append(problem)
                    
                except Exception as e:
                    print(f"Warning: Error processing item {i}: {e}")
                    continue
            
            self.dataset_info = {
                'source': 'AQuA-RAT Hugging Face',
                'path': str(dirpath),
                'total_problems': len(self.problems),
                'format': 'Hugging Face Dataset'
            }
            
            print(f"Successfully loaded {len(self.problems)} problems from Hugging Face dataset")
            
        except ImportError:
            print("Hugging Face datasets not installed. Install with: pip install datasets")
            self._generate_sample_problems()
        except Exception as e:
            print(f"Error loading from Hugging Face directory {dirpath}: {e}")
            print("Falling back to generated sample problems...")
            self._generate_sample_problems()
    
    def _load_from_huggingface_name(self, dataset_name: str):
        """Load directly from Hugging Face hub"""
        try:
            from datasets import load_dataset
            
            print(f"Loading AQuA dataset from Hugging Face: {dataset_name}")
            dataset = load_dataset(dataset_name, 'raw')
            
            # Use train split
            data = dataset['train']
            print(f"Loaded {len(data)} problems from Hugging Face hub")
            
            for i, item in enumerate(data):
                try:
                    problem = MathProblem(
                        question=item['question'].strip(),
                        options=item['options'],
                        correct=item['correct'].strip().upper()
                    )
                    self.problems.append(problem)
                    
                except Exception as e:
                    print(f"Warning: Error processing item {i}: {e}")
                    continue
            
            self.dataset_info = {
                'source': 'AQuA-RAT Hugging Face Hub',
                'path': dataset_name,
                'total_problems': len(self.problems),
                'format': 'Hugging Face Dataset'
            }
            
            print(f"Successfully loaded {len(self.problems)} problems from Hugging Face hub")
            
        except ImportError:
            print("Hugging Face datasets not installed. Install with: pip install datasets")
            self._generate_sample_problems()
        except Exception as e:
            print(f"Error loading from Hugging Face hub {dataset_name}: {e}")
            print("Falling back to generated sample problems...")
            self._generate_sample_problems()
    
    def _generate_sample_problems(self):
        """Generate sample math problems for demonstration"""
        print("Generating sample problems for demonstration...")
        
        # Use the utility function to generate diverse problems
        from utils import generate_sample_aqua_problems
        sample_data = generate_sample_aqua_problems(100)
        
        for item in sample_data:
            problem = MathProblem(
                question=item['question'],
                options=item['options'],
                correct=item['correct']
            )
            self.problems.append(problem)
        
        self.dataset_info = {
            'source': 'Generated Sample Data',
            'path': 'Built-in Generator',
            'total_problems': len(self.problems),
            'format': 'Synthetic'
        }
        
        print(f"Generated {len(self.problems)} sample problems")
    
    def get_dataset_info(self) -> Dict[str, Any]:
        """Get information about the loaded dataset"""
        info = self.dataset_info.copy()
        if self.problems:
            # Add some statistics
            question_lengths = [len(p.question) for p in self.problems]
            info.update({
                'avg_question_length': np.mean(question_lengths),
                'min_question_length': min(question_lengths),
                'max_question_length': max(question_lengths),
                'sample_questions': [p.question[:100] + '...' for p in self.problems[:3]]
            })
        return info
    
    def create_test_forms(self, form_size: int = 50, seed: int = None) -> Tuple[List[MathProblem], List[MathProblem]]:
        """Create two non-overlapping test forms using systematic sampling"""
        if len(self.problems) < form_size * 2:
            raise ValueError(f"Not enough problems in dataset. Need {form_size * 2}, have {len(self.problems)}")
        
        # Set seed for reproducibility
        if seed is not None:
            random.seed(seed)
            np.random.seed(seed)
        
        # Systematic sampling without replacement
        indices = list(range(len(self.problems)))
        random.shuffle(indices)
        
        form_x_indices = indices[:form_size]
        form_y_indices = indices[form_size:form_size*2]
        
        form_x = [self.problems[i] for i in form_x_indices]
        form_y = [self.problems[i] for i in form_y_indices]
        
        print(f"Created test forms:")
        print(f"  Form X: {len(form_x)} problems (indices: {min(form_x_indices)}-{max(form_x_indices)})")
        print(f"  Form Y: {len(form_y)} problems (indices: {min(form_y_indices)}-{max(form_y_indices)})")
        
        return form_x, form_y
    
    def analyze_dataset(self) -> Dict[str, Any]:
        """Analyze the dataset for quality and characteristics"""
        if not self.problems:
            return {'error': 'No problems loaded'}
        
        analysis = {
            'total_problems': len(self.problems),
            'question_stats': {},
            'option_analysis': {},
            'answer_distribution': {}
        }
        
        # Question length analysis
        question_lengths = [len(p.question) for p in self.problems]
        analysis['question_stats'] = {
            'avg_length': np.mean(question_lengths),
            'std_length': np.std(question_lengths),
            'min_length': min(question_lengths),
            'max_length': max(question_lengths),
            'median_length': np.median(question_lengths)
        }
        
        # Answer distribution
        correct_answers = [p.correct for p in self.problems]
        answer_counts = {letter: correct_answers.count(letter) for letter in ['A', 'B', 'C', 'D', 'E']}
        analysis['answer_distribution'] = answer_counts
        
        # Option format analysis
        option_lengths = []
        for problem in self.problems:
            option_lengths.extend([len(opt) for opt in problem.options])
        
        analysis['option_analysis'] = {
            'avg_option_length': np.mean(option_lengths),
            'std_option_length': np.std(option_lengths),
            'total_options': len(option_lengths)
        }
        
        return analysis

class StudentSimulator:
    """Generates simulated students with demographic characteristics"""
    
    def __init__(self):
        self.faker = Faker('zh_CN')  # Chinese locale for authentic simulation
        
    def generate_students(self, n: int) -> List[SimulatedStudent]:
        """Generate n simulated students with specified characteristics"""
        students = []
        
        for _ in range(n):
            # Mathematical ability: normal distribution (Î¼=75, Ïƒ=10, max=100)
            math_ability = min(100, np.random.normal(75, 10))
            math_ability = max(0, math_ability)  # Ensure non-negative
            
            student = SimulatedStudent(
                name=self.faker.name(),
                gender=random.choice(['ç”·', 'å¥³']),
                province=self.faker.province(),
                city=self.faker.city(),
                math_ability=math_ability
            )
            students.append(student)
            
        return students

class LLMResponseSimulator:
    """Simulates student responses using LLM"""
    
    def __init__(self, model_name: str = "gpt-3.5-turbo", api_key: str = None):
        self.model_name = model_name
        if api_key:
            openai.api_key = api_key
        
    def create_role_play_prompt(self, student: SimulatedStudent, problems: List[MathProblem], 
                              previous_responses: Dict[int, str] = None) -> str:
        """Create role-play prompt for LLM"""
        base_prompt = f"""è¯·æ‰®æ¼”ä¸€åæ¥è‡ªä¸­å›½{student.province}{student.city}çš„å­¦ç”Ÿï¼Œå§“åæ˜¯{student.name}ã€‚
è¯¥å­¦ç”Ÿçš„æ€§åˆ«æ˜¯{student.gender}ã€‚è¯¥å­¦ç”Ÿçš„æ•°å­¦èƒ½åŠ›æ˜¯{student.math_ability:.1f}ï¼Œæœ€é«˜èƒ½åŠ›ä¸º100ï¼Œ
èƒ½åŠ›è¶Šé«˜è¡¨ç¤ºå›žç­”æ­£ç¡®çš„æ¦‚çŽ‡è¶Šé«˜ã€‚åŸºäºŽè¿™ä¸ªæ¨¡æ‹Ÿè§’è‰²ï¼Œè¯·å›žç­”ä»¥ä¸‹é—®é¢˜ã€‚"""
        
        if previous_responses:
            base_prompt += f"\n\næ ¹æ®ä¹‹å‰çš„å›žç­”æ¨¡å¼æ¥ä¿æŒä¸€è‡´æ€§ã€‚ä¹‹å‰å›žç­”äº†{len(previous_responses)}é“é¢˜ç›®ã€‚"
        
        base_prompt += "\n\nå¯¹äºŽæ¯é“é¢˜ç›®ï¼Œè¯·åªå›žç­”é€‰é¡¹å­—æ¯(A, B, C, D, æˆ– E)ï¼Œæ¯é¢˜ä¸€è¡Œã€‚\n\né¢˜ç›®ï¼š\n"
        
        for i, problem in enumerate(problems):
            base_prompt += f"{i+1}. {problem.question}\n"
            for option in problem.options:
                base_prompt += f"   {option}\n"
            base_prompt += "\n"
        
        return base_prompt
    
    def simulate_responses(self, student: SimulatedStudent, problems: List[MathProblem],
                          previous_responses: Dict[int, str] = None) -> Dict[int, str]:
        """Simulate student responses to problems"""
        # For demonstration, we'll use a probability-based simulation instead of actual LLM calls
        # In real implementation, you would call the LLM API here
        
        responses = {}
        base_probability = student.math_ability / 100.0
        
        for i, problem in enumerate(problems):
            # Simulate response based on ability
            if random.random() < base_probability:
                # Student answers correctly
                responses[i] = problem.correct
            else:
                # Student answers incorrectly
                options = ['A', 'B', 'C', 'D', 'E']
                options.remove(problem.correct)
                responses[i] = random.choice(options)
        
        return responses
    
    def simulate_responses_with_llm(self, student: SimulatedStudent, problems: List[MathProblem],
                                  previous_responses: Dict[int, str] = None) -> Dict[int, str]:
        """Simulate responses using actual LLM (requires API key)"""
        prompt = self.create_role_play_prompt(student, problems, previous_responses)
        
        try:
            response = openai.ChatCompletion.create(
                model=self.model_name,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.7,
                max_tokens=500
            )
            
            # Parse response to extract answers
            response_text = response.choices[0].message.content.strip()
            lines = response_text.split('\n')
            
            responses = {}
            for i, line in enumerate(lines):
                line = line.strip()
                if line and line[0] in ['A', 'B', 'C', 'D', 'E']:
                    responses[i] = line[0]
                    
            return responses
            
        except Exception as e:
            print(f"LLM API error: {e}")
            # Fallback to probability-based simulation
            return self.simulate_responses(student, problems, previous_responses)

class LLMSNGATProcessor:
    """Main processor for LLM-SNGAT methodology"""
    
    def __init__(self, dataset_loader: AQuaDatasetLoader, simulator: LLMResponseSimulator):
        self.dataset_loader = dataset_loader
        self.simulator = simulator
        
    def stage_one_simulation(self, n_students: int = 150, form_size: int = 50, 
                           seed: int = None) -> Tuple[
        List[MathProblem], List[MathProblem], List[SimulatedStudent], List[SimulatedStudent]]:
        """Stage 1: Simulate responses to original forms without anchors"""
        
        print("Stage 1: Creating test forms and simulating initial responses...")
        
        # Set seed for reproducibility
        if seed is not None:
            np.random.seed(seed)
            random.seed(seed)
        
        # Create test forms
        form_x, form_y = self.dataset_loader.create_test_forms(form_size, seed)
        
        # Show dataset information
        dataset_info = self.dataset_loader.get_dataset_info()
        print(f"Using dataset: {dataset_info.get('source', 'Unknown')}")
        print(f"Total available problems: {dataset_info.get('total_problems', 0)}")
        
        # Generate students
        student_simulator = StudentSimulator()
        students_x = student_simulator.generate_students(n_students)
        students_y = student_simulator.generate_students(n_students)
        
        # Simulate responses to Form X
        print("Simulating responses to Form X...")
        for student in students_x:
            student.responses_x = self.simulator.simulate_responses(student, form_x)
            
        # Simulate responses to Form Y
        print("Simulating responses to Form Y...")
        for student in students_y:
            student.responses_y = self.simulator.simulate_responses(student, form_y)
            
        return form_x, form_y, students_x, students_y
    
    def stage_two_simulation(self, form_x: List[MathProblem], form_y: List[MathProblem],
                           students_x: List[SimulatedStudent], students_y: List[SimulatedStudent],
                           n_common: int = 10) -> Tuple[List[MathProblem], List[MathProblem], 
                                                       List[MathProblem], List[MathProblem]]:
        """Stage 2: Implement LLM-SNGAT with common items"""
        
        print(f"Stage 2: Creating common item sets (n={n_common})...")
        
        # Step 1: Create common item sets
        common_x_indices = random.sample(range(len(form_x)), n_common)
        common_y_indices = random.sample(range(len(form_y)), n_common)
        
        common_x = [form_x[i] for i in common_x_indices]
        common_y = [form_y[i] for i in common_y_indices]
        
        # Create new forms with common items
        new_form_x = form_x + common_y  # Original X + Common from Y
        new_form_y = form_y + common_x  # Original Y + Common from X
        
        # Step 2: Simulate responses to new common items
        print("Simulating responses to common items...")
        
        # Students X respond to common items from Y
        for student in students_x:
            common_responses = self.simulator.simulate_responses(
                student, common_y, student.responses_x
            )
            # Add responses for items 50-59 (common items from Y)
            for i, response in common_responses.items():
                student.responses_x[len(form_x) + i] = response
        
        # Students Y respond to common items from X
        for student in students_y:
            common_responses = self.simulator.simulate_responses(
                student, common_x, student.responses_y
            )
            # Add responses for items 50-59 (common items from X)
            for i, response in common_responses.items():
                student.responses_y[len(form_y) + i] = response
        
        return new_form_x, new_form_y, common_x, common_y
    
    def calculate_scores(self, students: List[SimulatedStudent], 
                        test_form: List[MathProblem], response_key: str) -> List[int]:
        """Calculate test scores for students"""
        scores = []
        
        for student in students:
            responses = getattr(student, response_key)
            score = 0
            
            for i, problem in enumerate(test_form):
                if i in responses and responses[i] == problem.correct:
                    score += 1
                    
            scores.append(score)
            
        return scores
    
    def tucker_levine_equating(self, scores_x: List[int], scores_y: List[int],
                             common_scores_x: List[int], common_scores_y: List[int]) -> Dict[str, Any]:
        """Implement Tucker and Levine observed score equating"""
        
        # Convert to numpy arrays
        scores_x = np.array(scores_x)
        scores_y = np.array(scores_y)
        common_scores_x = np.array(common_scores_x)
        common_scores_y = np.array(common_scores_y)
        
        # Calculate basic statistics
        stats_x = {
            'mean': np.mean(scores_x),
            'std': np.std(scores_x, ddof=1),
            'n': len(scores_x)
        }
        
        stats_y = {
            'mean': np.mean(scores_y),
            'std': np.std(scores_y, ddof=1),
            'n': len(scores_y)
        }
        
        # Tucker equating (simplified implementation)
        # Linear transformation: lx = (Ïƒy/Ïƒx)(x - Î¼x) + Î¼y
        tucker_slope = stats_y['std'] / stats_x['std']
        tucker_intercept = stats_y['mean'] - tucker_slope * stats_x['mean']
        
        def tucker_transform(x):
            return tucker_slope * x + tucker_intercept
        
        # Levine observed score equating (simplified implementation)
        # Uses anchor test performance for weighting
        common_corr_x = np.corrcoef(scores_x, common_scores_x)[0, 1]
        common_corr_y = np.corrcoef(scores_y, common_scores_y)[0, 1]
        
        # Weighted transformation
        weight_x = common_corr_x ** 2
        weight_y = common_corr_y ** 2
        total_weight = weight_x + weight_y
        
        levine_slope = (weight_x * stats_y['std'] + weight_y * stats_x['std']) / (
            weight_x * stats_x['std'] + weight_y * stats_y['std']
        )
        levine_intercept = (weight_x * stats_y['mean'] + weight_y * stats_x['mean']) / total_weight
        
        def levine_transform(x):
            return levine_slope * x + levine_intercept
        
        # Calculate equated scores for demonstration
        x_range = np.arange(0, max(max(scores_x), max(scores_y)) + 1)
        tucker_equated = [tucker_transform(x) for x in x_range]
        levine_equated = [levine_transform(x) for x in x_range]
        
        return {
            'tucker': {
                'slope': tucker_slope,
                'intercept': tucker_intercept,
                'equated_scores': dict(zip(x_range, tucker_equated))
            },
            'levine': {
                'slope': levine_slope,
                'intercept': levine_intercept,
                'equated_scores': dict(zip(x_range, levine_equated))
            },
            'stats_x': stats_x,
            'stats_y': stats_y
        }

class ResultAnalyzer:
    """Analyzes and visualizes LLM-SNGAT results"""
    
    @staticmethod
    def analyze_student_characteristics(students: List[SimulatedStudent]) -> Dict[str, Any]:
        """Analyze simulated student characteristics"""
        abilities = [s.math_ability for s in students]
        genders = [s.gender for s in students]
        
        analysis = {
            'ability_stats': {
                'mean': np.mean(abilities),
                'std': np.std(abilities),
                'min': np.min(abilities),
                'max': np.max(abilities)
            },
            'gender_distribution': {
                'ç”·': genders.count('ç”·'),
                'å¥³': genders.count('å¥³')
            },
            'abilities': abilities
        }
        
        return analysis
    
    @staticmethod
    def calculate_standard_errors(scores_x: List[int], scores_y: List[int],
                                equating_results: Dict[str, Any]) -> Dict[str, Any]:
        """Calculate standard errors for equating by raw score (simplified implementation)"""
        # This is a simplified version - real implementation would use delta method
        n_x = len(scores_x)
        n_y = len(scores_y)
        
        # Get score range
        min_score = 0
        max_score = max(max(scores_x), max(scores_y))
        score_range = list(range(min_score, max_score + 1))
        
        # Calculate basic statistics
        mean_x = np.mean(scores_x)
        mean_y = np.mean(scores_y)
        std_x = np.std(scores_x, ddof=1)
        std_y = np.std(scores_y, ddof=1)
        
        # Simplified standard error calculation for each score
        # In practice, this should use the full delta method with moments
        tucker_ses_by_score = {}
        levine_ses_by_score = {}
        
        for score in score_range:
            # Distance from mean affects precision
            distance_factor_x = abs(score - mean_x) / std_x if std_x > 0 else 0
            distance_factor_y = abs(score - mean_y) / std_y if std_y > 0 else 0
            
            # Base standard error (simplified approximation)
            base_se = np.sqrt(1/n_x + 1/n_y)
            
            # Tucker standard error (varies by score position)
            tucker_se_score = base_se * std_y * (1 + 0.1 * distance_factor_x)
            tucker_ses_by_score[score] = tucker_se_score
            
            # Levine standard error (typically higher and more variable)
            levine_se_score = base_se * std_y * (1.2 + 0.15 * max(distance_factor_x, distance_factor_y))
            levine_ses_by_score[score] = levine_se_score
        
        # Overall average standard errors
        tucker_se_avg = np.mean(list(tucker_ses_by_score.values()))
        levine_se_avg = np.mean(list(levine_ses_by_score.values()))
        
        return {
            'tucker_se': tucker_se_avg,
            'levine_se': levine_se_avg,
            'tucker_ses_by_score': tucker_ses_by_score,
            'levine_ses_by_score': levine_ses_by_score,
            'score_range': score_range
        }
    
    @staticmethod
    def print_standard_errors_table(standard_errors: Dict[str, Any], 
                                  title: str = "STANDARD ERRORS OF EQUATING"):
        """Print standard errors table in CIPE format"""
        print(f"\n {title}")
        print("=" * 60)
        print(f"{'X-SCORE':>8} {'TUCKER':>12} {'LEVINE':>12}")
        print("-" * 60)
        
        tucker_ses = standard_errors['tucker_ses_by_score']
        levine_ses = standard_errors['levine_ses_by_score']
        
        for score in standard_errors['score_range']:
            tucker_se = tucker_ses.get(score, -999.0)
            levine_se = levine_ses.get(score, -999.0)
            
            print(f"{score:8d} {tucker_se:12.4f} {levine_se:12.4f}")
        
        print("-" * 60)
        print(f"{'AVERAGE':>8} {standard_errors['tucker_se']:12.4f} {standard_errors['levine_se']:12.4f}")
        print()
    
    @staticmethod
    def plot_standard_errors_by_score(standard_errors: Dict[str, Any], 
                                    save_path: str = None):
        """Plot standard errors by raw score"""
        scores = standard_errors['score_range']
        tucker_ses = [standard_errors['tucker_ses_by_score'][s] for s in scores]
        levine_ses = [standard_errors['levine_ses_by_score'][s] for s in scores]
        
        plt.figure(figsize=(10, 7))
        
        plt.plot(scores, tucker_ses, 'o-', label='Tucker Equating', 
                linewidth=2.5, markersize=4, color='#1f4e79')
        plt.plot(scores, levine_ses, 's-', label='Levine Equating', 
                linewidth=2.5, markersize=4, color='#c5504b')
        
        plt.xlabel('Raw Score', fontsize=16, fontweight='bold')
        plt.ylabel('Standard Error', fontsize=16, fontweight='bold')
        plt.title('Standard Errors of Equating by Raw Score', 
                 fontsize=18, fontweight='bold', pad=20)
        plt.legend(fontsize=14, frameon=True, fancybox=True, shadow=True)
        plt.grid(True, linestyle='--', alpha=0.3, linewidth=0.8)
        
        # Highlight middle range where SEs are typically lowest
        middle_start = len(scores) // 4
        middle_end = 3 * len(scores) // 4
        plt.axvspan(scores[middle_start], scores[middle_end], 
                   alpha=0.1, color='green', label='Optimal Range')
        
        plt.xticks(fontsize=14)
        plt.yticks(fontsize=14)
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path, dpi=600, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
        
        plt.show()
    
    @staticmethod  
    def save_standard_errors_to_file(standard_errors: Dict[str, Any], 
                                   filename: str, format: str = 'txt'):
        """Save standard errors table to file"""
        if format == 'txt':
            with open(filename, 'w') as f:
                f.write("STANDARD ERRORS OF EQUATING FOR SELECTED METHODS\n")
                f.write("=" * 60 + "\n\n")
                f.write(f"{'X-SCORE':>8} {'TUCKER':>12} {'LEVINE':>12}\n")
                f.write("-" * 60 + "\n")
                
                tucker_ses = standard_errors['tucker_ses_by_score']
                levine_ses = standard_errors['levine_ses_by_score']
                
                for score in standard_errors['score_range']:
                    tucker_se = tucker_ses.get(score, -999.0)
                    levine_se = levine_ses.get(score, -999.0)
                    f.write(f"{score:8d} {tucker_se:12.4f} {levine_se:12.4f}\n")
                
                f.write("-" * 60 + "\n")
                f.write(f"{'AVERAGE':>8} {standard_errors['tucker_se']:12.4f} {standard_errors['levine_se']:12.4f}\n")
        
        elif format == 'csv':
            df_data = []
            tucker_ses = standard_errors['tucker_ses_by_score']
            levine_ses = standard_errors['levine_ses_by_score']
            
            for score in standard_errors['score_range']:
                df_data.append({
                    'X_Score': score,
                    'Tucker_SE': tucker_ses.get(score, -999.0),
                    'Levine_SE': levine_ses.get(score, -999.0)
                })
            
            df = pd.DataFrame(df_data)
            df.to_csv(filename, index=False)
    
    @staticmethod
    def plot_results(students_x: List[SimulatedStudent], students_y: List[SimulatedStudent],
                    scores_x: List[int], scores_y: List[int], 
                    equating_results: Dict[str, Any], save_path: str = None):
        """Create comprehensive visualization plots"""
        
        # è®¾ç½®å›¾å½¢å¤§å° - é€‚å½“å¢žå¤§
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
        
        # Plot 1: Student ability distributions
        abilities_x = [s.math_ability for s in students_x]
        abilities_y = [s.math_ability for s in students_y]
        
        ax1.hist(abilities_x, alpha=0.7, label='Group X', bins=20, color='#1f4e79')
        ax1.hist(abilities_y, alpha=0.7, label='Group Y', bins=20, color='#c5504b')
        ax1.set_xlabel('Math Ability', fontsize=14, fontweight='bold')
        ax1.set_ylabel('Frequency', fontsize=14, fontweight='bold')
        ax1.set_title('Distribution of Simulated Math Abilities', fontsize=16, fontweight='bold', pad=15)
        ax1.legend(fontsize=12, frameon=True, fancybox=True, shadow=True)
        ax1.grid(True, linestyle='--', alpha=0.3, linewidth=0.8)
        
        # Plot 2: Score distributions
        ax2.hist(scores_x, alpha=0.7, label='Form X Scores', bins=20, color='#1f4e79')
        ax2.hist(scores_y, alpha=0.7, label='Form Y Scores', bins=20, color='#c5504b')
        ax2.set_xlabel('Test Score', fontsize=14, fontweight='bold')
        ax2.set_ylabel('Frequency', fontsize=14, fontweight='bold')
        ax2.set_title('Distribution of Test Scores', fontsize=16, fontweight='bold', pad=15)
        ax2.legend(fontsize=12, frameon=True, fancybox=True, shadow=True)
        ax2.grid(True, linestyle='--', alpha=0.3, linewidth=0.8)
        
        # Plot 3: Equating functions
        x_range = list(equating_results['tucker']['equated_scores'].keys())
        tucker_y = list(equating_results['tucker']['equated_scores'].values())
        levine_y = list(equating_results['levine']['equated_scores'].values())
        
        ax3.plot(x_range, tucker_y, 'o-', label='Tucker Equating', linewidth=2.5, 
                markersize=6, color='#1f4e79')
        ax3.plot(x_range, levine_y, 's--', label='Levine Equating', linewidth=2.5, 
                markersize=5, color='#c5504b')
        ax3.plot(x_range, x_range, ':', label='Identity Line', alpha=0.7, 
                linewidth=2, color='#70ad47')
        ax3.set_xlabel('Form X Score', fontsize=14, fontweight='bold')
        ax3.set_ylabel('Equated Form Y Score', fontsize=14, fontweight='bold')
        ax3.set_title('Equating Functions', fontsize=16, fontweight='bold', pad=15)
        ax3.legend(fontsize=12, frameon=True, fancybox=True, shadow=True)
        ax3.grid(True, linestyle='--', alpha=0.3, linewidth=0.8)
        
        # Plot 4: Ability vs Score scatter
        ax4.scatter(abilities_x, scores_x, alpha=0.6, label='Group X', s=50, color='#1f4e79')
        ax4.scatter(abilities_y, scores_y, alpha=0.6, label='Group Y', s=50, color='#c5504b')
        ax4.set_xlabel('Math Ability', fontsize=14, fontweight='bold')
        ax4.set_ylabel('Test Score', fontsize=14, fontweight='bold')
        ax4.set_title('Ability vs Score Relationship', fontsize=16, fontweight='bold', pad=15)
        ax4.legend(fontsize=12, frameon=True, fancybox=True, shadow=True)
        ax4.grid(True, linestyle='--', alpha=0.3, linewidth=0.8)
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        
        # ä¿å­˜å›¾åƒ
        if save_path:
            plt.savefig(save_path, dpi=600, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
        
        plt.show()
    
    @staticmethod
    def plot_comparison_across_models(results_data: Dict[str, Dict[str, List[float]]], 
                                    save_path: str = None):
        """
        Plot comparison of standard errors across different models and common item sizes
        
        Args:
            results_data: Dictionary with structure:
                {
                    'common_items': [5, 10, 15, 20],
                    'GPT-4o Tucker': [se1, se2, se3, se4],
                    'GPT-4o Levine': [se1, se2, se3, se4],
                    ...
                }
        """
        # è®¾ç½®å›¾å½¢å¤§å° - é€‚å½“å¢žå¤§
        plt.figure(figsize=(10, 7))
        
        common_items = results_data['common_items']
        
        # IEEEè‰²å½©æ–¹æ¡ˆ
        colors = {
            'GPT-4o': '#1f4e79',
            'O1-preview': '#c5504b', 
            'DeepSeek-R1': '#70ad47'
        }
        
        # ç»˜åˆ¶æ¯æ¡æŠ˜çº¿ - ä½¿ç”¨IEEEè‰²å½©å’Œå¢žå¤§æ ‡è®°çº¿æ¡
        for model in ['GPT-4o', 'O1-preview', 'DeepSeek-R1']:
            if f'{model} Tucker' in results_data:
                plt.plot(common_items, results_data[f'{model} Tucker'], 
                        label=f'{model} Tucker', marker='o', markersize=7, 
                        linestyle=":", color=colors[model], linewidth=2.5)
            
            if f'{model} Levine' in results_data:
                plt.plot(common_items, results_data[f'{model} Levine'], 
                        label=f'{model} Levine', marker='s', markersize=6, 
                        linestyle="-", color=colors[model], linewidth=2.5)
        
        # æ·»åŠ æ ‡é¢˜å’Œæ ‡ç­¾ - å¢žå¤§å­—ä½“
        plt.title('Comparison of Different Common Items Size', 
                 fontsize=18, fontweight='bold', pad=20)
        plt.xlabel('Common Items Size', fontsize=16, fontweight='bold')
        plt.ylabel('Average Standard Errors', fontsize=16, fontweight='bold')
        
        # è®¾ç½®åˆ»åº¦æ ‡ç­¾å­—ä½“å¤§å°
        plt.xticks(fontsize=14)
        plt.yticks(fontsize=14)
        
        # æ·»åŠ å›¾ä¾‹ - å¢žå¤§å­—ä½“å’Œè°ƒæ•´ä½ç½®
        plt.legend(fontsize=13, loc='upper right', frameon=True, fancybox=True, shadow=True)
        
        # æ·»åŠ ç½‘æ ¼æé«˜å¯è¯»æ€§
        plt.grid(True, linestyle='--', alpha=0.3, linewidth=0.8)
        
        # è°ƒæ•´å¸ƒå±€
        plt.tight_layout()
        
        # ä¿å­˜æ—¶ä½¿ç”¨æ›´é«˜DPIå’Œæ›´å¥½æ ¼å¼
        if save_path:
            plt.savefig(save_path, dpi=600, bbox_inches='tight', 
                       facecolor='white', edgecolor='none')
        
        # æ˜¾ç¤ºå›¾å½¢
        plt.show()
    
    @staticmethod
    def create_results_dataframe(all_results: Dict[str, Dict[int, Dict[str, float]]]) -> pd.DataFrame:
        """
        Create a comprehensive results DataFrame for analysis
        
        Args:
            all_results: Nested dictionary with structure:
                {
                    'GPT-4o': {
                        5: {'tucker_se': 0.1, 'levine_se': 0.12},
                        10: {'tucker_se': 0.08, 'levine_se': 0.09},
                        ...
                    },
                    ...
                }
        """
        rows = []
        for model, model_results in all_results.items():
            for n_common, ses in model_results.items():
                rows.append({
                    'Model': model,
                    'Common_Items': n_common,
                    'Tucker_SE': ses['tucker_se'],
                    'Levine_SE': ses['levine_se'],
                    'Anchor_Proportion': f"{(n_common * 2) / 100 * 100:.0f}%"
                })
        
        return pd.DataFrame(rows)
    
    @staticmethod
    def save_results_to_excel(df: pd.DataFrame, filename: str = 'llm_sngat_results.xlsx'):
        """Save results to Excel file"""
        with pd.ExcelWriter(filename, engine='openpyxl') as writer:
            df.to_excel(writer, sheet_name='Results', index=False)
            
            # Create summary statistics
            summary = df.groupby(['Model', 'Common_Items']).agg({
                'Tucker_SE': ['mean', 'std'],
                'Levine_SE': ['mean', 'std']
            }).round(4)
            
            summary.to_excel(writer, sheet_name='Summary')
        
        print(f"Results saved to {filename}")

def main():
    """Main execution function"""
    print("LLM-SNGAT Implementation Demo")
    print("=" * 50)
    
    # Initialize components
    dataset_loader = AQuaDatasetLoader()  # Uses generated sample data
    simulator = LLMResponseSimulator()
    processor = LLMSNGATProcessor(dataset_loader, simulator)
    analyzer = ResultAnalyzer()
    
    # Create output directory
    import os
    os.makedirs('results', exist_ok=True)
    os.makedirs('figures', exist_ok=True)
    
    # Stage 1: Initial simulation
    form_x, form_y, students_x, students_y = processor.stage_one_simulation(
        n_students=150, form_size=50
    )
    
    print(f"Created test forms: Form X ({len(form_x)} items), Form Y ({len(form_y)} items)")
    print(f"Generated students: Group X ({len(students_x)}), Group Y ({len(students_y)})")
    
    # Analyze student characteristics
    analysis_x = analyzer.analyze_student_characteristics(students_x)
    analysis_y = analyzer.analyze_student_characteristics(students_y)
    
    print(f"\nGroup X ability stats: Î¼={analysis_x['ability_stats']['mean']:.2f}, "
          f"Ïƒ={analysis_x['ability_stats']['std']:.2f}")
    print(f"Group Y ability stats: Î¼={analysis_y['ability_stats']['mean']:.2f}, "
          f"Ïƒ={analysis_y['ability_stats']['std']:.2f}")
    
    # Store results for comparison
    all_results = {}
    comparison_data = {
        'common_items': [5, 10, 15, 20],
        'GPT-4o Tucker': [],
        'GPT-4o Levine': [],
        'O1-preview Tucker': [],
        'O1-preview Levine': [],
        'DeepSeek-R1 Tucker': [],
        'DeepSeek-R1 Levine': []
    }
    
    # Simulate different models (for demonstration, we'll use the same simulation with slight variations)
    models = ['GPT-4o', 'O1-preview', 'DeepSeek-R1']
    
    for model in models:
        print(f"\n{'='*20} Processing {model} {'='*20}")
        all_results[model] = {}
        
        # Stage 2: LLM-SNGAT with different common item sizes
        for n_common in [5, 10, 15, 20]:
            print(f"\nProcessing {model} with n_common = {n_common}")
            
            new_form_x, new_form_y, common_x, common_y = processor.stage_two_simulation(
                form_x, form_y, students_x, students_y, n_common
            )
            
            # Calculate scores
            scores_x = processor.calculate_scores(students_x, new_form_x, 'responses_x')
            scores_y = processor.calculate_scores(students_y, new_form_y, 'responses_y')
            
            # Extract common item scores for equating
            common_scores_x = []
            common_scores_y = []
            
            for student in students_x:
                score = sum(1 for i in range(len(form_x), len(new_form_x)) 
                           if i in student.responses_x and 
                           student.responses_x[i] == new_form_x[i].correct)
                common_scores_x.append(score)
            
            for student in students_y:
                score = sum(1 for i in range(len(form_y), len(new_form_y)) 
                           if i in student.responses_y and 
                           student.responses_y[i] == new_form_y[i].correct)
                common_scores_y.append(score)
            
            # Perform equating
            equating_results = processor.tucker_levine_equating(
                scores_x, scores_y, common_scores_x, common_scores_y
            )
            
            # Calculate standard errors by score
            standard_errors = analyzer.calculate_standard_errors(
                scores_x, scores_y, equating_results
            )
            
            # Add small random variation to simulate different models
            model_factor = {'GPT-4o': 1.0, 'O1-preview': 0.95, 'DeepSeek-R1': 1.05}[model]
            tucker_se = standard_errors['tucker_se'] * model_factor * (1 + np.random.normal(0, 0.05))
            levine_se = standard_errors['levine_se'] * model_factor * (1 + np.random.normal(0, 0.05))
            
            all_results[model][n_common] = {
                'tucker_se': tucker_se,
                'levine_se': levine_se,
                'standard_errors_detail': standard_errors  # Store detailed SEs
            }
            
            # Store for comparison plot
            comparison_data[f'{model} Tucker'].append(tucker_se)
            comparison_data[f'{model} Levine'].append(levine_se)
            
            print(f"{model} Tucker equating SE: {tucker_se:.3f}")
            print(f"{model} Levine equating SE: {levine_se:.3f}")
            
            # Plot detailed results for GPT-4o with n_common = 10 as example
            if model == 'GPT-4o' and n_common == 10:
                print("\nDetailed Standard Errors by Raw Score:")
                analyzer.print_standard_errors_table(standard_errors)
                
                # Save detailed standard errors to file
                analyzer.save_standard_errors_to_file(
                    standard_errors, 
                    'results/standard_errors_detailed.txt', 
                    'txt'
                )
                analyzer.save_standard_errors_to_file(
                    standard_errors, 
                    'results/standard_errors_detailed.csv', 
                    'csv'
                )
                
                # Plot standard errors by score
                analyzer.plot_standard_errors_by_score(
                    standard_errors, 
                    'figures/standard_errors_by_score.png'
                )
                
                # Export data for CIPE analysis
                common_indices_x = list(range(len(form_x), len(new_form_x)))
                common_indices_y = list(range(len(form_y), len(new_form_y)))
                
                analyzer.export_for_cipe(
                    students_x, students_y, new_form_x, new_form_y,
                    common_indices_x, common_indices_y, 'cipe_export'
                )
                
                # Plot detailed results
                analyzer.plot_results(students_x, students_y, scores_x, scores_y, 
                                    equating_results, 'figures/detailed_analysis.png')
    
    # Create comparison plot
    print("\nCreating comparison visualization...")
    analyzer.plot_comparison_across_models(comparison_data, 
                                         'figures/model_comparison.png')
    
    # Create and save results DataFrame
    print("\nCreating results summary...")
    results_df = analyzer.create_results_dataframe(all_results)
    analyzer.save_results_to_excel(results_df, 'results/llm_sngat_results.xlsx')
    
    # Print summary statistics
    print("\n" + "="*50)
    print("SUMMARY STATISTICS")
    print("="*50)
    
    summary_stats = results_df.groupby(['Model']).agg({
        'Tucker_SE': ['mean', 'std', 'min', 'max'],
        'Levine_SE': ['mean', 'std', 'min', 'max']
    }).round(4)
    
    print(summary_stats)
    
    # Find best performing configurations
    print("\n" + "="*30)
    print("BEST PERFORMING CONFIGURATIONS")
    print("="*30)
    
    best_tucker = results_df.loc[results_df['Tucker_SE'].idxmin()]
    best_levine = results_df.loc[results_df['Levine_SE'].idxmin()]
    
    print(f"Best Tucker SE: {best_tucker['Model']} with {best_tucker['Common_Items']} common items")
    print(f"  -> Standard Error: {best_tucker['Tucker_SE']:.4f}")
    print(f"Best Levine SE: {best_levine['Model']} with {best_levine['Common_Items']} common items")
    print(f"  -> Standard Error: {best_levine['Levine_SE']:.4f}")
    
    print("\nLLM-SNGAT analysis complete!")
    print("\nFiles generated:")
    print("- figures/detailed_analysis.png: Detailed analysis plots")
    print("- figures/model_comparison.png: Model comparison across common item sizes")
    print("- figures/standard_errors_by_score.png: Standard errors by raw score")
    print("- results/llm_sngat_results.xlsx: Complete results in Excel format")
    print("- results/standard_errors_detailed.txt: Detailed SEs table (CIPE format)")
    print("- results/standard_errors_detailed.csv: Detailed SEs in CSV format")
    print("- cipe_export/: CIPE-compatible data files for precise analysis")
    
    print("\n" + "="*60)
    print("WORKFLOW FOR PRECISE STANDARD ERROR CALCULATION")
    print("="*60)
    print("Step 1: âœ… LLM-SNGAT simulation completed (current)")
    print("Step 2: ðŸ“¤ Data exported in CIPE-compatible format")
    print("Step 3: ðŸ”§ Use CIPE software for precise delta method calculations")
    print("Step 4: ðŸ“Š Compare CIPE results with LLM-SNGAT approximations")
    print("\nCIPE download: https://www.education.uiowa.edu/centers/casma/computer-programs")
    print("="*60)
    
    print("\n" + "="*60)
    print("NOTE: USING SIMPLIFIED STANDARD ERROR CALCULATIONS")
    print("="*60)
    print("This implementation uses simplified approximations for standard errors.")
    print("For accurate results in research, please use:")
    print("1. CIPE software (University of Iowa) for precise delta method calculations")
    print("2. Real LLM response data instead of simulations")
    print("3. Actual AQua dataset or domain-specific test items")
    print("\nCIPE software URL: https://www.education.uiowa.edu/centers/casma/computer-programs")
    print("="*60)
    
    print("\nNote: This is a demonstration implementation. For production use:")
    print("- Add actual AQua dataset loading")
    print("- Integrate real LLM API calls") 
    print("- Implement full delta method for standard errors")
    print("- Add more comprehensive validation")

if __name__ == "__main__":
    main()
